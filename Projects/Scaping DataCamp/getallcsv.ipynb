{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping csv files from DataCamp\n",
    "\n",
    "Over time I've seen a lot of requests for getting all of the dataset from DataCamp in 1 eazy place. This project uses Beautiful Soup and regex to find all of the .csv files on the DataCamp site and download them to a local folder. It is possible not the most effecient or elegant way to do it but it seems to work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## manually download a csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "url='https://assets.datacamp.com/production/course_2072/datasets/company-stock-movements-2010-2015-incl.csv'\n",
    "y=url.split('/')\n",
    "destination='Data/Csv1/'+(y[-1])\n",
    "df=pd.read_csv(url,sep=',')\n",
    "df.to_csv(destination)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get the links to courses\n",
    "\n",
    "Initially we will use Beautiful Soup to look at the courses folder on datacamp and bring back the html text. It will then extract all of the a_tags to find the links containing courses/ extract the href then to list courses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# Specify url\n",
    "url = 'https://campus.datacamp.com/courses/unsupervised-learning-in-python/clustering-for-dataset-exploration?ex=1'\n",
    "# Package the request, send the request and catch the response: r\n",
    "r = requests.get(url)\n",
    "# Extracts the response as html: html_doc\n",
    "html_doc = r.text\n",
    "\n",
    "# create a BeautifulSoup object from the HTML: soup\n",
    "soup = BeautifulSoup(html_doc, \"lxml\")\n",
    "\n",
    "# Find all 'a' tags (which define hyperlinks): a_tags\n",
    "a_tags=soup.find_all('a')\n",
    "\n",
    "# Extract the /courses pages\n",
    "courses=[]\n",
    "pattern = re.compile(\"/courses//*\")\n",
    "for link in a_tags:\n",
    "    match = re.findall(pattern, str(link))\n",
    "    if match:\n",
    "        courses.append(link.get('href'))  \n",
    "# print(courses) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get all the links from courses to .csv files\n",
    "\n",
    "We pretty much repeat the previous steps, except this time instead of looking at one page we look at the list of course pages we created in the first step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "csv=[]\n",
    "for course in courses:    \n",
    "    # print(course) \n",
    "    \n",
    "    # Specify url\n",
    "    url = \"https://campus.datacamp.com/courses/unsupervised-learning-in-python/clustering-for-dataset-exploration?ex=1\"+course    \n",
    "    # Package the request, send the request and catch the response: r\n",
    "    r = requests.get(url)\n",
    "    # Extracts the response as html: html_doc\n",
    "    html_doc = r.text\n",
    "\n",
    "    # create a BeautifulSoup object from the HTML: soup\n",
    "    soup = BeautifulSoup(html_doc, \"lxml\")\n",
    "    \n",
    "    # Find all 'a' tags (which define hyperlinks): a_tags\n",
    "    a_tags1=soup.find_all('a')\n",
    "    \n",
    "    pattern = re.compile(\"csv/*\")    \n",
    "    # append the URLs to list\n",
    "    for link in a_tags1:\n",
    "        match = re.findall(pattern, str(link))\n",
    "        if match:\n",
    "            csv.append(link.get('href'))\n",
    "print(csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get the csv's  and save locally\n",
    "\n",
    "We are going to use pandas pd.read_csv() and df.to_csv() methods to take the file and save it to a local folder. We'll also create a text file to document what we've done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "codebook='Data/Csv/codebook.txt'\n",
    "\n",
    "for file in csv:\n",
    "    y=file.split('/')\n",
    "    destination='Data/Csv/'+(y[-1])\n",
    "    df=pd.read_csv(file)\n",
    "    df.to_csv(destination)\n",
    "\n",
    "    with open(codebook, \"a\") as f:\n",
    "        f.write(\"source : \" + file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it all together\n",
    "\n",
    "In this notebook i split out each element of the process so that i could ensure each element work before moving onto the next. Now i will but it all into a single snipet of scripy that i ca save as a .py file to share and run when required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 csv files checked!\n"
     ]
    }
   ],
   "source": [
    "### A python script to collect all of the .csv files from DataCamp ###\n",
    "### and copy them to a folder named Data in the currect directory. ###\n",
    "### Created 3/3/18 by David coxon ###\n",
    "\n",
    "# Import packages\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import datetime\n",
    "import pandas as pd\n",
    "\n",
    "codebook='Data/Csv/importedcsv.txt'\n",
    "now = datetime.datetime.now()\n",
    "now = str(now)\n",
    "# Specify url\n",
    "url = 'https://campus.datacamp.com/courses/unsupervised-learning-in-python/'\n",
    "# Package the request, send the request and catch the response: r\n",
    "r = requests.get(url)\n",
    "html_doc = r.text\n",
    "\n",
    "# create a BeautifulSoup object from the HTML: soup\n",
    "soup = BeautifulSoup(html_doc, \"lxml\")\n",
    "\n",
    "# Find all 'a' tags (which define hyperlinks): a_tags\n",
    "a_tags=soup.find_all('a')\n",
    "\n",
    "# Extract the /courses pages\n",
    "courses=[]\n",
    "pattern = re.compile(\"/courses//*\")\n",
    "for link in a_tags:\n",
    "    match = re.findall(pattern, str(link))\n",
    "    if match:\n",
    "        courses.append(link.get('href'))   \n",
    "\n",
    "# iterate over list of courses getting links to csv files\n",
    "csv=[]     \n",
    "for course in courses:    \n",
    "    \n",
    "    # Specify url\n",
    "    url = \"https://campus.datacamp.com/courses/unsupervised-learning-in-python/\"  \n",
    "    # Package the request, send the request and catch the response: r\n",
    "    r = requests.get(url)\n",
    "    # Extracts the response as html: html_doc\n",
    "    html_doc = r.text\n",
    "\n",
    "    # create a BeautifulSoup object from the HTML: soup\n",
    "    soup = BeautifulSoup(html_doc, \"lxml\")\n",
    "    \n",
    "    # Find all 'a' tags (which define hyperlinks): a_tags\n",
    "    a_tags1=soup.find_all('a')\n",
    "\n",
    "    # append the URLs to csv list\n",
    "    pattern = re.compile(\"csv/*\")        \n",
    "    for link in a_tags1:\n",
    "        match = re.findall(pattern, str(link))\n",
    "        if match:\n",
    "            csv.append(link.get('href')) \n",
    "\n",
    "# open Csv files and copy to Local device           \n",
    "for file in csv:\n",
    "    y=file.split('/')\n",
    "    destination='Data/Csv/'+(y[-1])\n",
    "    df=pd.read_csv(file)\n",
    "    df.to_csv(destination)\n",
    "\n",
    "    # Document which files were collected\n",
    "    with open(codebook, \"a\") as f:\n",
    "        f.write(\"\\nsource : \" + file + \" dated collected : \" + now)\n",
    "print(str(len(csv)) + \" csv files checked!\")"
   ]
  },
  {
   "attachments": {
    "datacamp.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "![datacamp.png](attachment:datacamp.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
